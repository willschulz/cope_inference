================================================================================
CoPE-A-9B CONTENT CLASSIFICATION API
================================================================================

SERVICE ENDPOINT
----------------
Tailscale IP: 100.97.88.105  (PREFERRED - use this from datascience.manx-celsius.ts.net)
GCP Internal IP: 10.162.0.3
Port: 8000

Base URL: http://100.97.88.105:8000

Tailscale hostname (if MagicDNS enabled): http://coper-v1.manx-celsius.ts.net:8000


================================================================================
ENDPOINTS
================================================================================

1. HEALTH CHECK
---------------
GET /health

Response:
{
  "status": "healthy",
  "model_loaded": true,
  "gpu_available": true,
  "gpu_name": "NVIDIA L4",
  "gpu_memory_allocated_gb": 6.11
}


2. SINGLE ITEM CLASSIFICATION
-----------------------------
POST /label
Content-Type: application/json

Request:
{
  "policy": "<policy text>",
  "content": "<text to classify>"
}

Response:
{
  "label": 0,           // 0 = does not match policy, 1 = matches policy
  "raw_output": "0",    // raw model output token
  "confidence": 0.97,   // probability of the chosen token (0 or 1)
  "prob_positive": 0.03 // probability of label=1 (positive class)
}


3. BATCH CLASSIFICATION (RECOMMENDED FOR BULK)
----------------------------------------------
POST /batch
Content-Type: application/json

Request:
{
  "policy": "<policy text - applied to ALL items>",
  "items": [
    {"id": "tweet_123", "content": "First tweet text"},
    {"id": "tweet_456", "content": "Second tweet text"},
    ...
  ]
}

Response:
{
  "results": [
    {"id": "tweet_123", "label": 0, "raw_output": "0", "confidence": 0.98, "prob_positive": 0.02},
    {"id": "tweet_456", "label": 1, "raw_output": "1", "confidence": 0.94, "prob_positive": 0.94},
    {"id": "tweet_789", "label": -1, "raw_output": "", "confidence": 0.0, "prob_positive": 0.0, "error": "Prompt too long (2083 tokens, max 2040)"},
    ...
  ],
  "processed": 2,
  "elapsed_seconds": 1.2,
  "items_per_second": 1.67
}

RESPONSE FIELDS:
- label: Binary classification (0 or 1), or -1 if item was skipped
- raw_output: The raw token output from the model
- confidence: Probability the model assigned to its chosen answer (0.0-1.0)
- prob_positive: Probability of label=1, useful for ranking or custom thresholds
- error: (optional) Error message if item was skipped (e.g., prompt too long)

SKIPPED ITEMS:
Items with prompts exceeding 2040 tokens are skipped and returned with:
- label: -1 (sentinel value)
- error: Description of why the item was skipped
- processed count excludes skipped items

Check for skipped items: [r for r in results if r.get("error")]

BATCH LIMITS:
- No hard limit on batch size (vLLM handles memory management)
- Recommended: 100-500 items per request for optimal throughput
- Results returned in same order as input
- IDs are preserved for matching back to source data


================================================================================
POLICY FORMAT (CoPE Specification)
================================================================================

Policies must follow this structure:

# Criteria

## Overview
[Brief description of what this policy classifies]

## Definition of Terms
- Term1: Definition
- Term2: Definition

## Interpretation of Language
- [Guidance for ambiguous cases]

## Definition of Labels

### (LABEL_CODE): Label Name

#### Includes
- Criteria1: What qualifies content for this label
- Criteria2: Another qualifying characteristic

#### Excludes
- Exception1: What does NOT qualify even if similar
- Exception2: Another exception


================================================================================
RECOMMENDED USAGE FOR 2M TWEETS
================================================================================

THROUGHPUT: ~100-120 items/second with batching
ESTIMATED TIME: ~5-6 hours for 2M tweets

SERVICE: vLLM backend with systemd (starts automatically on VM boot)

OPTIMIZATION STRATEGIES:

1. USE BATCH ENDPOINT WITH LARGE BATCHES
   - Send 100-500 items per request for optimal throughput
   - vLLM processes items in parallel using continuous batching
   - Larger batches = higher throughput (up to ~120 items/sec)

2. PARALLEL REQUESTS
   - 2-4 concurrent batch requests can improve throughput further
   - vLLM handles request queuing efficiently

3. CHECKPOINT PROGRESS
   - Save results frequently (every 10K items)
   - Track processed IDs to resume on failure


PYTHON CLIENT EXAMPLE:
----------------------
import requests
from concurrent.futures import ThreadPoolExecutor
import json

API_URL = "http://100.97.88.105:8000"  # Tailscale IP
POLICY = """... your policy text ..."""

def classify_batch(items):
    """Classify a batch of items. items = [{"id": ..., "content": ...}, ...]"""
    response = requests.post(
        f"{API_URL}/batch",
        json={"policy": POLICY, "items": items},
        timeout=300  # 5 min timeout for large batches
    )
    response.raise_for_status()
    return response.json()

def process_tweets(tweets, batch_size=100, workers=2):
    """
    tweets = [{"id": "...", "content": "..."}, ...]
    """
    results = []
    batches = [tweets[i:i+batch_size] for i in range(0, len(tweets), batch_size)]
    
    with ThreadPoolExecutor(max_workers=workers) as executor:
        for batch_result in executor.map(classify_batch, batches):
            results.extend(batch_result["results"])
            print(f"Processed {len(results)}/{len(tweets)} "
                  f"({batch_result['items_per_second']:.1f} items/sec)")
    
    return results

# Usage:
# tweets = [{"id": str(i), "content": row["text"]} for i, row in df.iterrows()]
# results = process_tweets(tweets)

# Handling results:
# - Check for skipped items: skipped = [r for r in results if r.get("error")]
# - Get successful results: success = [r for r in results if r["label"] != -1]
# - Filter uncertain predictions: [r for r in results if r["confidence"] < 0.7]
# - Sort by positive probability: sorted(results, key=lambda r: r["prob_positive"], reverse=True)
# - Custom threshold: label = 1 if r["prob_positive"] > 0.3 else 0


CURL EXAMPLE:
-------------
curl -X POST http://100.97.88.105:8000/batch \
  -H "Content-Type: application/json" \
  -d '{
    "policy": "# Criteria\n\n## Definition of Labels\n\n### (HS): Hate Speech\n\n#### Includes\n- Slurs\n- Dehumanization",
    "items": [
      {"id": "1", "content": "Hello world"},
      {"id": "2", "content": "Test message"}
    ]
  }'


================================================================================
TROUBLESHOOTING
================================================================================

1. Connection refused
   - Check service is running: curl http://100.97.88.105:8000/health
   - Verify Tailscale is connected: tailscale status
   - Check you can ping: ping 100.97.88.105

2. Timeout errors
   - Increase timeout (batch of 100 can take 30-60 seconds)
   - Reduce batch size if memory issues

3. 503 Service Unavailable
   - Model still loading (takes ~2-3 min after service start)
   - Check status: sudo systemctl status cope-vllm
   - Check logs: sudo journalctl -u cope-vllm -f

4. Slow performance
   - Expect ~100-120 items/sec with batching
   - If seeing <10 items/sec, ensure you're using batch endpoint with 100+ items
   - Longer content = slightly slower (more tokens to process)

5. Items returning label=-1 with error
   - Content + policy combined exceeds 2040 token limit
   - Consider shortening policy or filtering very long content


================================================================================
SERVICE MANAGEMENT (systemd)
================================================================================

Service VM Tailscale IP: 100.97.88.105 (coper-v1)
Service VM GCP Internal IP: 10.162.0.3
Service name: cope-vllm

The service starts automatically when the VM boots and cleanly releases
GPU memory on shutdown/restart.

COMMANDS (SSH to VM):
---------------------
Check status:     sudo systemctl status cope-vllm
Start service:    sudo systemctl start cope-vllm
Stop service:     sudo systemctl stop cope-vllm
Restart service:  sudo systemctl restart cope-vllm
View logs:        sudo journalctl -u cope-vllm -f
View recent logs: sudo journalctl -u cope-vllm -n 100

INSTALLATION (already done):
----------------------------
cd ~/cope_inference && sudo ./install_service.sh

UNINSTALL (revert to manual scripts):
-------------------------------------
cd ~/cope_inference && sudo ./uninstall_service.sh


================================================================================
DEPRECATED
================================================================================

The following are deprecated and should NOT be used:

- cope_service.py        (old non-vLLM backend, ~10x slower)
- start.sh               (manual start for old backend)
- start_vllm.sh          (manual start, replaced by systemd service)
- stop.sh                (manual stop, replaced by systemd service)
- cope.log               (logs now in journald, use journalctl)
- cope.pid               (not used with systemd)

================================================================================
